{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Img_3-DeepMindExp_LunarLander-v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ViW-Sp9nLC5n",
        "colab_type": "code",
        "outputId": "13f60d87-a493-4068-bf37-076a8d811b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "!pip install gym\n",
        "!pip install box2d-py\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!sudo apt-get install xvfb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K    100% |████████████████████████████████| 450kB 10.4MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/39/37/f285403a09cc261c56b6574baace1bdcf4b8c7428c8a7239cbba137bc0eb/PyVirtualDisplay-0.2.1.tar.gz\n",
            "Collecting EasyProcess (from pyvirtualdisplay)\n",
            "  Downloading https://files.pythonhosted.org/packages/45/3a/4eecc0c7995a13a64739bbedc0d3691fc574245b7e79cff81905aa0c2b38/EasyProcess-0.2.5.tar.gz\n",
            "Building wheels for collected packages: pyvirtualdisplay, EasyProcess\n",
            "  Building wheel for pyvirtualdisplay (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/8c/16/1c64227974ae29c687e4cc30fd691d5c0fd40f54446dde99da\n",
            "  Building wheel for EasyProcess (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/41/22/19/af15ef6264c58b625a82641ed7483ad05e258fbd8925505227\n",
            "Successfully built pyvirtualdisplay EasyProcess\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.2.5 pyvirtualdisplay-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 783 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.2 [783 kB]\n",
            "Fetched 783 kB in 2s (340 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.2_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8nYy_JHbLC5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "gym.logger.set_level(40)\n",
        "\n",
        "env = gym.make('LunarLander-v2').unwrapped\n",
        "env.seed(0)\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhcDWLhxLC5t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"next_state\", \"reward\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "\n",
        "        e = self.experience(state, action, next_state, reward, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
        "                                  \n",
        "        return (states, actions, next_states, rewards, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RkX8mPKoLC5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w):\n",
        "      \n",
        "        super(DQN, self).__init__()  \n",
        "        self.layer1   = nn.Sequential(nn.Conv2d(3, 16, 5, stride=2),\\\n",
        "                                      nn.BatchNorm2d(16),\\\n",
        "                                      nn.ReLU())\n",
        "        \n",
        "        self.layer2   = nn.Sequential(nn.Conv2d(16, 32, 5, stride=2),\\\n",
        "                                      nn.BatchNorm2d(32),\n",
        "                                      nn.ReLU())\n",
        "        \n",
        "        self.layer3   = nn.Sequential(nn.Conv2d(32, 32, 5, stride=2),\\\n",
        "                                      nn.BatchNorm2d(32),\\\n",
        "                                      nn.ReLU(),\\\n",
        "                                      nn.MaxPool2d(kernel_size=2))\n",
        "        \n",
        "        self.layer4   = nn.Sequential(nn.Dropout() ,\\\n",
        "                                      nn.Linear(3585, 1024),\\\n",
        "                                      nn.ReLU()) \n",
        "        \n",
        "        self.layer5   = nn.Sequential(nn.Linear(1024, 4),\\\n",
        "                                      nn.Softmax())\n",
        "        \n",
        "        self.layer_a  = nn.Sequential(nn.Linear(4, 1),\\\n",
        "                                      nn.ReLU()) \n",
        "        \n",
        "    def forward(self, x, a):\n",
        "#         print(x, a)\n",
        "        \n",
        "        out = self.layer1(x.to(device))\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        \n",
        "        a   = self.layer_a(torch.FloatTensor(np.array(a)/4).to(device))\n",
        "        if(a.shape[0]==BATCH_SIZE):\n",
        "            out = torch.cat((out, a), 1)\n",
        "        else:\n",
        "            out = torch.cat((out[0], a), 0)\n",
        "            \n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VGRSOwgvLC5x",
        "colab_type": "code",
        "outputId": "e3eb4eb1-3d80-46a5-e0ec-4bfb1c7544c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(100, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def get_screen():\n",
        "    \n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    \n",
        "    \n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    new_screen = resize(screen)\n",
        "    \n",
        "    fin_screen = new_screen[:, :85]\n",
        "\n",
        "    return fin_screen.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "screen = get_screen()\n",
        "plt.figure()\n",
        "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAErCAYAAACfA4lXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUb4AANiM8gUAwGaULwAANqN8AQCw2f8DCMDFwVKLyWAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7YXAzlcOLC51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 500\n",
        "TARGET_UPDATE = 1\n",
        "SEED = 0\n",
        "MEMORY_SIZE = int(10e5)\n",
        "TAU = 0.05\n",
        "POLYAK = 1 - TAU\n",
        "LR = .0016\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "init_screen = get_screen()\n",
        "state = torch.cat([init_screen, init_screen, init_screen], 2)\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "policy_net = DQN(screen_height, screen_width).to(device)\n",
        "target_net = DQN(screen_height, screen_width).to(device)\n",
        "\n",
        "try:\n",
        "  policy_net.load_state_dict(torch.load(\"./policy.nn\", map_location=device))\n",
        "except:\n",
        "  print(\"There isnt old weights\")\n",
        "  \n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), LR)\n",
        "memory = ReplayMemory(MEMORY_SIZE, BATCH_SIZE, SEED)\n",
        "\n",
        "def select_action(state, i, actions):\n",
        "    sample = random.random()\n",
        "    \n",
        "    if(i<10):\n",
        "        return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
        "      \n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                    math.exp(-1. * i / EPS_DECAY)\n",
        "    if sample > eps_threshold :\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state, actions).max(0)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
        "      \n",
        "def good_action(state, actions):\n",
        "    with torch.no_grad():\n",
        "        # t.max(1) will return largest column value of each row.\n",
        "        # second column on max result is index of where max element was\n",
        "        # found, so we pick action with the larger expected reward.\n",
        "        return policy_net(state, actions).max(0)[1].view(1, 1)\n",
        "      \n",
        "def test():\n",
        "    test_rewards = []\n",
        "    for i in range(100):\n",
        "        env.reset()\n",
        "        current_screen = get_screen()\n",
        "    \n",
        "        state = current_screen - current_screen\n",
        "    \n",
        "        done = False\n",
        "        reward = 0\n",
        "        rew_per_episode = 0\n",
        "        for j in count():\n",
        "          \n",
        "            action = good_action(state)\n",
        "            _, reward, done, _ = env.step(action.item())\n",
        "        \n",
        "            rew_per_episode += reward\n",
        "\n",
        "            # Observe new state\n",
        "            last_screen = current_screen\n",
        "            current_screen = get_screen()\n",
        "\n",
        "            if not done:\n",
        "                next_state = last_screen - current_screen\n",
        "            else:\n",
        "                next_state = state\n",
        "            memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            if done:\n",
        "                break\n",
        "        test_rewards.append(rew_per_episode)       \n",
        "    return np.mean(test_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TsUPaTP0LC53",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    \n",
        "    experiences = memory.sample()\n",
        "    states, actions4, next_states, rewards, dones = experiences\n",
        "    \n",
        "    actions = actions4.reshape(4, 64)[0]\n",
        "\n",
        "    Q_argmax = target_net(torch.FloatTensor(next_states).to(device), actions4).detach()\n",
        "    _, a_prime = Q_argmax.max(1)\n",
        "  \n",
        "    Q_targets_next = target_net(torch.FloatTensor(next_states).to(device), actions4).detach().gather(1, a_prime.unsqueeze(1))\n",
        "    \n",
        "    Q_targets = torch.FloatTensor(rewards).to(device) + (GAMMA * Q_targets_next * (1 - torch.FloatTensor(dones).to(device)))\n",
        "  \n",
        "    Q_expected = policy_net( torch.FloatTensor(states).to(device), actions4)\n",
        "    \n",
        "    loss = F.mse_loss(Q_targets, Q_expected.gather(1, torch.LongTensor(actions.reshape(64,1)).to(device)))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "wUG3RJbALC57",
        "colab_type": "code",
        "outputId": "cf12d4a6-b490-4568-a177-2759bd5bdae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "num_episodes = 100\n",
        "passed = 0\n",
        "plt.figure()\n",
        "reward_sum = 0\n",
        "rew_max = -999999\n",
        "lr=LR\n",
        "tracked_rewards = []\n",
        "for i_episode in count():\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    screen1 = get_screen()\n",
        "    screen2 = screen1\n",
        "    screen3 = screen2\n",
        "    actions = [0 ,0, 0, 0]\n",
        "    state = torch.cat([screen1, screen2, screen3], 2).cpu()\n",
        "    \n",
        "    done = False\n",
        "    reward = 0\n",
        "    rew_per_episode = 0\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state, i_episode, actions)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        \n",
        "        rew_per_episode += reward\n",
        "\n",
        "        # Observe new state\n",
        "        screen4 = get_screen()\n",
        "        \n",
        "        if not done:\n",
        "            next_state = torch.cat([screen2, screen3, screen4], 2).cpu()\n",
        "        else:\n",
        "            next_state = state\n",
        "        \n",
        "        actions = [actions[1],\\\n",
        "                   actions[2],\\\n",
        "                   actions[3],\\\n",
        "                   action.item()]\n",
        "        \n",
        "        memory.push(state.cpu(), actions, next_state, reward, done)\n",
        "        \n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        screen1 = screen2\n",
        "        screen2 = screen3\n",
        "        screen3 = screen4\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        if(BATCH_SIZE<=len(memory)):\n",
        "            optimize_model()\n",
        "        if done:\n",
        "            tracked_rewards.append(rew_per_episode)\n",
        "            break\n",
        "            \n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "        if(t%4==0):\n",
        "            for param, target_param in zip(policy_net.parameters(), target_net.parameters()):\n",
        "                target_param.data.copy_( (POLYAK * target_param.data) + ((1 - POLYAK) * param.data))\n",
        "    \n",
        "    print('\\rEpisode {} \\tLearning rate {:.6f} \\tAverage Score: {:.2f}'.format(i_episode+1, lr, np.mean(tracked_rewards)), end=\"\")\n",
        "    \n",
        "    if((i_episode+1)%100==0):\n",
        "      \n",
        "        print('\\rEpisode {} \\tLearning rate {:.6f} \\tAverage Score: {:.2f}'.format(i_episode+1, lr, np.mean(tracked_rewards)))\n",
        "\n",
        "        torch.save(policy_net.state_dict(), \"policy.nn\")\n",
        "        \n",
        "        lr=LR/(i_episode+1)*100\n",
        "        optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "\n",
        "        if(np.mean(tracked_rewards)>=195):\n",
        "            test_count = 0\n",
        "            for seed in [135, 6589, 5]:\n",
        "                env.seed(seed)\n",
        "                test_mark = test()\n",
        "                print(test_mark)\n",
        "                if(test_mark>=195):\n",
        "                    test_count+=1\n",
        "                if(test_count>2):\n",
        "                    break\n",
        "        tracked_rewards = []\n",
        "env.close()\n",
        "plt.figure\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 100 \tLearning rate 0.001600 \tAverage Score: -165.61\n",
            "Episode 200 \tLearning rate 0.001600 \tAverage Score: -155.34\n",
            "Episode 252 \tLearning rate 0.000800 \tAverage Score: -165.31"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "awiTvnkiLC5-",
        "colab_type": "code",
        "outputId": "89dc4f8a-6b5d-49d8-86e0-9224a7621a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(tracked_rewards))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76ad6cdc386e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracked_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dpH4_ePFLC6A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def good_action(state):\n",
        "    with torch.no_grad():\n",
        "        # t.max(1) will return largest column value of each row.\n",
        "        # second column on max result is index of where max element was\n",
        "        # found, so we pick action with the larger expected reward.\n",
        "        return policy_net(state).max(1)[1].view(1, 1)\n",
        "      \n",
        "def test():\n",
        "    test_rewards = []\n",
        "    for i in range(100):\n",
        "        env.reset()\n",
        "        current_screen = get_screen()\n",
        "    \n",
        "        state = current_screen - current_screen\n",
        "    \n",
        "        done = False\n",
        "        reward = 0\n",
        "        rew_per_episode = 0\n",
        "        for j in count():\n",
        "          \n",
        "            action = good_action(state)\n",
        "            _, reward, done, _ = env.step(action.item())\n",
        "        \n",
        "            rew_per_episode += reward\n",
        "\n",
        "            # Observe new state\n",
        "            last_screen = current_screen\n",
        "            current_screen = get_screen()\n",
        "\n",
        "            if not done:\n",
        "                next_state = last_screen - current_screen\n",
        "            else:\n",
        "                next_state = state\n",
        "    \n",
        "            memory.push(state, action, next_state, reward, done)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            if done:\n",
        "                break\n",
        "        test_rewards.append(rew_per_episode)       \n",
        "    return np.mean(test_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mjKgkkB_1DG",
        "colab_type": "code",
        "outputId": "235c311a-4b78-4b98-f80d-734bdf350098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-434.45062135944676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "MZL6kZ0pCgo6",
        "colab_type": "code",
        "outputId": "881c197d-e691-4c62-dc5c-60953ddca6f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "IMkbaYSyDQfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}